AWS CLI:
---------------
https://awscli.amazonaws.com/AWSCLIV2.msi

Putty:
--------
https://the.earth.li/~sgtatham/putty/latest/w64/putty-64bit-0.81-installer.msi

VS Code:
------------
https://code.visualstudio.com/download


aws ssm send-command --document-name "AWS-RunShellScript" --document-version "1" --targets '[{"Key":"InstanceIds","Values":[]}]' --parameters '{"workingDirectory":[""],"executionTimeout":["3600"],"commands":["echo \"ho bro\""]}' --timeout-seconds 600 --max-concurrency "50" --max-errors "0" --region us-east-1

on linux:
----------------
sudo yum update

 sudo yum install tree unzip


AWS CLI Install:
--------------------
curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
unzip awscliv2.zip
sudo ./aws/install

aws --version

sudo yum install gnupg*

Converged Infra:
----------------------
https://www.vmware.com/topics/glossary/content/converged-infrastructure.html#:~:text=Converged%20infrastructure%20is%20a%20pre,components%20separately%20from%20different%20suppliers.

12 factor apps:
----------------------
https://12factor.net/

Docker in Windows:
-----------------------------

set-executionpolicy unrestricted

Invoke-WebRequest -UseBasicParsing "https://raw.githubusercontent.com/microsoft/Windows-Containers/Main/helpful_tools/Install-DockerCE/install-docker-ce.ps1" -o install-docker-ce.ps1

.\install-docker-ce.ps1

docker --version

[Net.ServicePointManager]::SecurityProtocol = [Net.SecurityProtocolType]::Tls12

Start-BitsTransfer -Source "https://github.com/docker/compose/releases/download/v2.28.1/docker-compose-windows-x86_64.exe" 
ls

cp .\docker-compose-windows-x86_64.exe 'C:\Windows\System32\docker-compose.exe'

docker-compose version

---------------------------------

Linux Docker:
-----------------
sudo yum install -y yum-utils
	
sudo yum-config-manager --add-repo https://download.docker.com/linux/rhel/docker-ce.repo
sudo yum install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin

sudo systemctl start docker

sudo docker --version

sudo docker compose version

-------------------------------------------
Demo1:
---------
39  sudo docker images
   40  sudo docker ps -a
   41  sudo docker pull httpd
   42  sudo docker images
   43  sudo docker ps -a
   44  sudo docker run -d -p 7070:80 --name con1 httpd:latest
   45  sudo docker ps -a
Browse the page

   46  sudo docker stop con1

Browse the page
   47  sudo docker start con1

Browse the page
   48  sudo docker run -d -p 7777:80 --name con2 httpd:latest
   49  sudo docker ps -a
 Browse the page
   51  sudo docker rm -f con1
   52  sudo docker rm -f con2
   53  sudo docker ps -a
   54  sudo docker images
   55  sudo docker rmi httpd:latest
   56  sudo docker images
---------------------------------------------------

sudo docker pull mcr.microsoft.com/windows/servercore/iis


3rd July:
---------------
sudo systemctl start docker
Demo1:
------------
90  sudo docker pull nginx
   91  sudo docker run -d -p 7777:80 --name con1 nginx:latest
   92  sudo docker ps -a
   93  sudo docker exec -it con1 sh
# pwd

# cd /usr/share/nginx/html
# ls

# echo "<h1>Hello Nginx</h1>" > index.html
# exit


Demo2:
---------
 106  sudo docker run -it --name con1 alpine:latest
 
#ip a
#env
#exit

  108  sudo docker rm -f con1


Volume Demo:
-------------------
sudo su -
docker volume list
   
    3  docker volume create vol1cat 
    4  docker volume list
    5  cd /var/lib/docker/volumes/
    6  ls
    7  tree vol1/
    8  sudo docker run -d -p 7777:80 --name con1 -v vol1:/usr/local/apache2/htdocs/ httpd:latest
    9  tree vol1/
   10  cat vol1/_data/index.html
   11  sudo docker exec -it con1 sh
# cd htdocs
# echo "<h1>Volume Example</h1>" > index.html
# exit


   12  cat vol1/_data/index.html
   13  echo "<h1>Volume Example-- Modified outside</h1>" > vol1/_data/index.html
   14  sudo docker exec -it con1 sh

# pwd

# cd htdocs
# cat index.html

# exit

NW Demo1:
---------------
 3  sudo docker network list
    4  sudo docker inspect network bridge
    5  sudo docker inspect network host
   
    7  sudo docker run -d -p 7777:80 --name con1 nginx
    8  sudo docker inspect network host
    9  sudo docker inspect network bridge
   10  sudo docker rm -f con1
   11  sudo docker inspect network bridge


---------------
NW Demo2:
----------------
 2  sudo docker network list
    3  sudo docker inspect network bridge
    4  ifconfig
    5  sudo yum install net-tools
    6  ifconfig
    7  sudo docker network create --driver bridge --subnet 10.0.0.0/16 mybridge
    8  sudo docker network list
    9  sudo docker inspect network mybridge
   10  ifconfig
   11  sudo docker run -d --name con1 busybox sh -c "while true;do sleep 3600;done"
   12  sudo docker inspect network mybridge
   13  sudo docker network connect mybridge con1
   14  sudo docker inspect network mybridge
   15  sudo docker run -d --name con2 busybox sh -c "while true;do sleep 3600;done"
   16  sudo docker inspect network mybridge
   17  sudo docker network connect mybridge con2
   18  sudo docker inspect network mybridge
   19  sudo docker exec -it con1 ip a
   20  sudo docker exec -it con2 ip a
   21  sudo docker exec -it con1 ping 10.0.0.3
   22  sudo docker exec -it con2 ping 10.0.0.2
   23  sudo docker exec -it con1 ping con2
   24  sudo docker exec -it con2 ping con1
   25  ifconfig
   26  sudo docker exec -it con1 ping 172.17.0.1
   27  sudo docker exec -it con2 ping 172.17.0.1
   28  sudo docker rm -f con1
   29  sudo docker rm -f con2
   30  sudo docker network rm mybridge
   31  sudo docker network list


NW Demo3:
-----------------
 1  ifconfig
    2  sudo docker network list
    3  sudo docker inspect network host
    4  sudo docker run -d --name con1 --network host busybox sh -c "while true;do sleep 3600;done"
    5  sudo docker inspect network host
    6  sudo docker exec -it con1 ip a
    7  sudo docker rm -f
    8  sudo docker rm -f con1
    9  sudo docker inspect network host
   10  sudo docker run -d --name con1 --network host nginx
   11  curl http://localhost
   12  sudo docker rm -f con1

PubDemo:
----------------
Create a pulic repo in hub.docker.com
34  sudo docker login
   35  sudo docker images
   36  sudo docker tag mysaticappimg:latest papacrao/staticrepo1:mysaticappimg
   37  sudo docker images
   38  sudo docker push papacrao/statsupicrepo1:mysaticappimg
   39  sudo docker rmi papacrao/staticrepo1:mysaticappimg
   40  sudo docker images
   41  sudo docker rmi mysaticappimg:latest
   42  sudo docker images+-

---------------
ECR Demo:
---------------
Create public ECR repo.

46  sudo docker images
   47  sudo docker pull papacrao/staticrepo1:mysaticappimg
   48  sudo docker images
   49  sudo docker tag papacrao/staticrepo1:mysaticappimg public.ecr.aws/q5z6o6s6/staticrepo1:mysaticappimg
   50  sudo docker images
   51  aws ecr-public get-login-password --region us-east-1 | sudo docker login --username AWS --password-stdin public.ecr.aws/q5z6o6s6
   52  sudo docker push public.ecr.aws/q5z6o6s6/staticrepo1:mysaticappimg



Lab document
=====================

https://matst1tr12345.s3.amazonaws.com/docker.zip

JDK:
---------
https://aka.ms/download-jdk/microsoft-jdk-11.0.23-windows-x64.msi

Eclipse:
------------
https://www.eclipse.org/downloads/download.php?file=/technology/epp/downloads/release/2022-09/R/eclipse-jee-2022-09-R-win32-x86_64.zip
https://www.eclipse.org/downloads/download.php?file=/technology/epp/downloads/release/2021-09/R/eclipse-jee-2021-09-R-win32-x86_64.zip

Docker Labs:
------------------
https://matst1tr12345.s3.amazonaws.com/docker.zip

4th July:
------------
Demo1:
-----------
FROM maven AS stage1
COPY ./src /usr/src/app/src
COPY pom.xml /usr/src/app
RUN mvn -f /usr/src/app/pom.xml package

FROM tomcat
RUN rm -fr /usr/local/tomcat/webapps/ROOT
COPY --from=stage1 /usr/src/app/target/demoapp1.war /usr/local/tomcat/webapps/ROOT.war

composedemo1:
---------------------
173  mkdir demo2
  174  cd demo2
  175  vi docker-compose.yaml
------------------------------
version: '3.4'

services:
  frontend:
    image: nginx
    ports:
      - 7070:80

  backend:
    image: httpd
    ports:
      - 7777:80
---------------------------



  176  sudo docker images
  177  sudo docker ps -a
  178  sudo docker compose up -d
  
  182  sudo docker compose ps -a
  183  sudo docker ps -a
  184  sudo docker compose down


cdemo2:
--------------
version: '3.7'

services:
  webui:
    image: staticwebui
    build:
      context: .
      dockerfile: ./Dockerfile
    ports:
      - 7070:80

cdemo3:
----------
version: '3.7'

services:
  webui:
    image: nginx
    ports:
      - "7777-7779:80"
    deploy:
      replicas: 3

------------
cdemo4:
------------
version: '3.7'

services:
  service1:
    image: nginx
    ports:
      - 7778:80
    networks:
      - testnw

  service2:
    image: nginx
    ports:
      - 7777:80
    
    networks:
      - testnw


networks:
  testnw: 
    driver: bridge
    ipam:
      config:
        - subnet: 10.0.0.0/24
          gateway: 10.0.0.1


-----------
cdemo5:
---------------
version: '3.4'

services:
  service1:
    image: nginx
    ports:
      - 7777:80
    volumes:
      - vol1:/usr/share/nginx/html
    
   

  service2:
    image: nginx
    ports:
      - 7778:80
    volumes:
      - vol1:/usr/share/nginx/html

volumes:
  vol1:
    driver: local

------------
cdemo6:
------------
version: "3.7"

services:
  app:
    image: nginx
    
    ports:
      - 7777:80
    
    
    volumes:
      - ./app:/usr/share/nginx/html


==============================
Kubernetes

------------
Kubectl install:

curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
 
curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl.sha256"
 
echo "$(cat kubectl.sha256)  kubectl" | sha256sum --check
 
sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl
 
chmod +x kubectl
mkdir -p ~/.local/bin
mv ./kubectl ~/.local/bin/kubectl

sudo docker stop $(sudo docker ps -aq)
https://kubernetes.io/docs/tasks/tools/install-kubectl-linux/

helm install
==================

$ curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3
$ chmod 700 get_helm.sh
$ ./get_helm.sh
$ helm version
https://helm.sh/docs/intro/install/


terraform install
================
https://developer.hashicorp.com/terraform/install

sudo yum install -y yum-utils
sudo yum-config-manager --add-repo https://rpm.releases.hashicorp.com/RHEL/hashicorp.repo
sudo yum -y install terraform

tf-eks demo:
-------------
cd ~
curl -LO https://matst1tr12345.s3.amazonaws.com/tfeks.zip
unzip tfeks.zip
cd tfeks
vi main.tf
(Enter AccessKey and Sec Acckey)
terraform init
terraform plan
terraform apply

 140  aws eks update-kubeconfig --name tesco-cluster1
  141  cat .kube/config
    143  kubectl config current-context
  144  kubectl cluster-info
  145  kubectl get nodes

aws eks update-kubeconfig --name tesco-cluster1
sudo docker rmi -f $(sudo docker images -aq)

5th July:
------------------
Demo 1: Interaction with pods
  200  kubectl get nodes
  201  kubectl get pods
  202  kubectl run pod1 --image=public.ecr.aws/q5z6o6s6/staticrepo1:mysaticappimg
  203  kubectl get pod
  204  kubectl describe pod pod1
  205  clear
  206  kubectl get pod pod1
  207  kubectl get pod pod1 -o yaml > pod1.yaml
  208  cat pod1.yaml
  209  kubectl delete pod pod1
  210  kubectl get pods
  211  kubectl run pod1 --image=public.ecr.aws/q5z6o6s6/staticrepo1:mysaticappimg
  212  kubectl get pods
  213  kubectl get pod -o wide
  214  kubectl delete pod pod1

demo2:
----------
 228  kubectl get pod
  229  kubectl get replicaset
  230  kubectl get deployment
  231  kubectl create deployment dep1 --image=public.ecr.aws/q5z6o6s6/staticrepo1:mysaticappimg --replicas=3
  232  kubectl get pod
  233  kubectl get replicaset
  234  kubectl get deployment
  235  kubectl get all
  236  kubectl scale --current-replicas=3 --replicas=5 deployment.apps/dep1
  237  kubectl get all
  238  kubectl get pod -o wide
  239  kubectl scale --current-replicas=5 --replicas=3 deployment.apps/dep1
  240  kubectl get pod -o wide
  241  kubectl delete deployment dep1
  242  kubectl get all

Demo3:
------------
245  kubectl get all
  246  kubectl create deployment dep1 --image=public.ecr.aws/q5z6o6s6/staticrepo1:mysaticappimg --replicas=3
  247  kubectl get all
  248  kubectl expose deployment dep1 --type=Loadbalancer --port=80
  249  kubectl expose deployment dep1 --type=LoadBalancer --port=80
  250  kubectl get all
  251  kubectl delete service dep1
  252  kubectl get all
  253  kubectl delete deployment dep1
  254  kubectl get all

On duplicate linux session:
------------------------------------
watch -x kubectl get all

Demo4:
-----------
 257  mkdir kdemos
  258  cd kdemos
  259  mkdir demo1
  260  cd demo1
  261  clear
  262  kubectl create deployment dep1 --image=public.ecr.aws/q5z6o6s6/staticrepo1:mysaticappimg --replicas=3 --dry-run -o yaml > dep1.yaml
  263  ls
  264  cat dep1.yaml
  265  kubectl apply -f dep1.yaml
  266  vi dep1.yaml
  267  kubectl apply -f dep1.yaml
  268  vi dep1.yaml
  269  kubectl apply -f dep1.yaml
  270  ls
  271  kubectl expose deployment dep1 --type=LoadBalancer --port=80 --dry-run -o yaml > service1.yaml
  272  ls
  273  cat service1.yaml
  274  kubectl apply -f service1.yaml
  275  kubectl delete -f service1.yaml
  276  kubectl delete -f dep1.yaml
  277  clear
  278  touch app1.yaml
  279  cat dep1.yaml
  280  vi app1.yaml
  281  cat service1.yaml
  282  vi app1.yaml
  283  clear
  284  ls
  285  kubectl apply -f app1.yaml
  286  kubectl delete -f app1.yaml

Demo5:
----------
304  kubectl get namespace
  305  kubectl create namespace ns1
  306  kubectl get namespace
  307  ls
  308  kubectl apply -f dep1.yaml --namespace ns1
  309  kubectl delete -f dep1.yaml  --namespace ns1
  310  kubectl apply -f dep1.yaml --namespace ns1
  311  kubectl delete namespace ns1
  312  kubectl create namespace ns1 --dry-run -o yaml > ns1.yaml
  313  ls
 
  315  cat ns1.yaml
  316  history
 
  318   kubectl create deployment dep1 --image=public.ecr.aws/q5z6o6s6/staticrepo1:mysaticappimg --replicas=3 --namespace ns1 --dry-run -o yaml > dep4.yaml
  319  cat dep4.yaml
  320  vi app2.yaml
  321  cat ns1.yaml
  322  vi app2.yaml

  324  kubectl apply -f app2.yaml
  325  kubectl delete -f app2.yaml

Demo6:
-------------
kubectl create namespace ns1

rq.yaml:
------
apiVersion: v1
kind: ResourceQuota
metadata:
  name: mem-quota-demo
spec:
  hard:
    
    requests.memory: 1Gi
    
    limits.memory: 2Gi
	
kubectl apply -f rq.yaml --namespace ns1
	
kubectl get ResourceQuota --namespace ns1

kubectl describe ResourceQuota mem-quota-demo --namespace ns1

pod1.yaml:
-----------
apiVersion: v1
kind: Pod
metadata:
  name: quota-mem-cpu-demo
spec:
  containers:
  - name: quota-mem-cpu-demo-ctr
    image: nginx
    resources:
      limits:
        memory: "2Gi"
        
      requests:
        memory: "1Gi"
        

 kubectl apply -f pod1.yaml --namespace ns1
 kubectl describe ResourceQuota mem-quota-demo --namespace ns1
 
 pod2.yaml:
 ------------
apiVersion: v1
kind: Pod
metadata:
  name: quota-mem-cpu-demo-2
spec:
  containers:
  - name: quota-mem-cpu-demo-2-ctr
    image: redis
    resources:
      limits:
        memory: "1Gi"
        
      requests:
        memory: "700Mi"
        


kubectl apply -f pod2.yaml --namespace ns1

 kubectl delete namespace ns1


demo7:
---------
 7  kubectl get nodes
vi pod1.yaml
-------------------
apiVersion: v1
kind: Pod
metadata:
  name: pod1
spec:
  containers:
  - name: quota-mem-cpu-demo-ctr
    image: nginx
----------------------------
    8  kubectl apply -f pod1.yaml
    9  kubectl get pod -o wide
 14  kubectl describe node ip-10-0-1-106.ec2.internal
   15  kubectl label node ip-10-0-1-106.ec2.internal "dept=trng"
   16  kubectl describe node ip-10-0-1-106.ec2.internal
vi p2.yaml
---------------
apiVersion: v1
kind: Pod
metadata:
  name: pod1
spec:
  containers:
  - name: quota-mem-cpu-demo-ctr
    image: nginx
  nodeSelector:
    dept: trng
---------------------
27  kubectl apply -f p2.yaml
   28  kubectl get pod -o wide
   29  kubectl delete -f p2.yaml
   30  kubectl label node ip-10-0-1-106.ec2.internal dept-

31  kubectl apply -f p2.yaml
   32  kubectl get pod -o wide
   33  kubectl label node ip-10-0-1-106.ec2.internal dept=trng
   34  kubectl get pod -o wide
   35  kubectl delete -f p2.yaml
   36  kubectl label node ip-10-0-1-106.ec2.internal dept-


demo8:
-----------
40  vi p2.yaml
(remove nodeSelector section)
   41  kubectl apply -f p2.yaml
   42  kubectl get pod
   43  kubectl label pod pod1 "env=dev"
   44  kubectl get pod --show-labels
   45  kubectl label pod pod1 "env=qa"
   46  kubectl label pod pod1 "env=qa"  --overwrite
   47  kubectl get pod --show-labels


Demo9:
--------------
kubectl run pod1 --image=public.ecr.aws/q5z6o6s6/staticrepo1:mysaticappimg
kubectl get pod
kubectl port-forward pod/pod1 7777:80

==================================
Demo 10
==================================

Linux Session3:
---------------
watch -x curl http://lb-hostname


Linux Session2:
----------------
watch -x kubectl get pod,service,replicaset




Linux Session1:
------------------
dep1.yaml:
------------

apiVersion: apps/v1
kind: Deployment
metadata:
  name: hello-dep
  namespace: default
spec:
  replicas: 5
  selector:
    matchLabels:
      app: hello-dep 

  template:
    metadata:
      labels:
        app: hello-dep
    spec:
      containers:
      - image: gcr.io/google-samples/hello-app:1.0
        imagePullPolicy: Always
        name: hello-dep
        ports:
        - containerPort: 8080
		
kubectl apply -f dep1.yaml
kubectl expose deployment hello-dep --type=LoadBalancer --port=80 --target-port=8080

		
Once you apply this or edit this, notice that there maybe a little downtime on your application because the old pods are getting terminated and the new ones are getting created.
The fix is pretty easy actually. This happens because kubernetes doesn’t know when your new pod is ready to start accepting requests, so as soon as your new pod gets created, the old pod is terminated without waiting to see if all the necessary services, processes have started in the new pod which would then enable it to receive requests.

To do this, Kubernetes provide a config option in deployment called Readiness Probe. Readiness Probe makes sure that the new pods created are ready to take on requests before terminating the old pods.



dep2.yaml:
---------

apiVersion: apps/v1
kind: Deployment
metadata:
  name: hello-dep
  namespace: default
spec:
  replicas: 5
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 25%
  selector:
    matchLabels:
      app: hello-dep
  template:
    metadata:
      labels:
        app: hello-dep
    spec:
      containers:
      - image: gcr.io/google-samples/hello-app:2.0
        imagePullPolicy: Always
        name: hello-dep
        ports:
        - containerPort: 8080
        readinessProbe:
          httpGet:
             path: /
             port: 8080
          initialDelaySeconds: 5
          periodSeconds: 5
          successThreshold: 1
			 
kubectl apply -f dep2.yaml



kubectl rollout status deployment hello-dep

kubectl rollout history deployment hello-dep

kubectl rollout history deployment hello-dep --revision 2

kubectl rollout undo deployments hello-dep

kubectl rollout undo deployment hello-dep --to-revision 1

kubectl rollout pause deployment hello-dep

kubectl rollout resume deployment hello-dep


DEmo 11
==========================================

------------------
Demo11:
-----------
Blue/Green Deployment:
---------------------

Start with a new Deployment and service.
Deploy new version deployment
Issue a health check
If health check passes, update load balancer and remove old deployment
If health check fails, stop and send  alert

blue.yaml:
------------
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-1.10
spec:
  selector:
    matchLabels:
      app: nginx-1.10
  

  replicas: 3
  template:
    metadata:
      labels:
        name: nginx
        version: "1.10"
        app: nginx-1.10
    spec:
      containers: 
        - name: nginx
          image: nginx:1.10
          ports:
            - name: http
              containerPort: 80
			  
bservice.yaml:
---------------
apiVersion: v1
kind: Service
metadata: 
  name: nginx
  labels: 
    name: nginx
spec:
  ports:
    - name: http
      port: 80
      targetPort: 80
  selector: 
    name: nginx
    version: "1.10"
  type: LoadBalancer
  
  

  
green.yaml:
-----------
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-1.11
spec:
  selector:
    matchLabels:
      app: nginx-1.11
  

  replicas: 3
  template:
    metadata:
      labels:
        name: nginx
        version: "1.11"
        app: nginx-1.11
    spec:
      containers: 
        - name: nginx
          image: nginx:1.11
          ports:
            - name: http
              containerPort: 80
			  
To cut over to the "green" deployment we will update the selector for the service. Edit the service.yaml and change the selector version to "1.11" and apply. That will make it so that it matches the pods on the "green" deployment.




Cloud and MS design patterns:
-------------------------------------------
https://learn.microsoft.com/en-us/azure/architecture/patterns/


08-july-2024
=================

Demo1(Static Provisioning): 2gb volume will be used from all the worker nodes
----------
302  mkdir stdemo
  303  cd stdemo
  304  vi pv.yaml
  305  kubectl get pv
  306  kubectl get pvc
  307  kubectl apply -f pv.yaml
--------------
kind: PersistentVolume
apiVersion: v1
metadata:
   name: pv0001
   labels:
      type: local
spec:

  capacity:
      storage: 2Gi
  accessModes:
      - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  claimRef:
    name: myclaim-1
    namespace: default
  hostPath:
         path: "/tmp/data01"
-----------------

  308  kubectl get pvc
  309  kubectl get pv
  310  vi pvc.yaml
---------------
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
   name: myclaim-1
spec:
   accessModes:
      - ReadWriteOnce
   resources:
      requests:
         storage: 2Gi

---------------
  311  kubectl apply -f pvc.yaml
  312  kubectl get pv
  313  kubectl get pvc
  314  vi pod.yaml
  ---------------
kind: Pod
apiVersion: v1
metadata:
   name: mypod
   labels:
      name: frontendhttp
spec:
   containers:
   - name: myfrontend
     image: nginx
     ports:
      - containerPort: 80
        name: "http-server"
     volumeMounts:
      - mountPath: "/usr/share/nginx/html"
        name: vol1
   volumes:
      - name: vol1
        persistentVolumeClaim:
          claimName: myclaim-1

----------------
  317  kubectl apply -f pod.yaml
  318  kubectl get pod
  319  kubectl describe pod mypod
  320  kubectl delete -f pod.yaml
  321  kubectl delete -f pvc.yaml
  322  kubectl delete -f pv.yaml

Demo2(Dynamic Provisioning): ebs volume will be created and used 

------------------
kind: Pod
apiVersion: v1
metadata:
   name: mypod
   labels:
      name: frontendhttp
spec:
   containers:
   - name: myfrontend
     image: nginx
     ports:
      - containerPort: 80
        name: nginx
     volumeMounts:
      - mountPath: "/usr/share/nginx/html"
        name: vol1
   volumes:
      - name: vol1
        persistentVolumeClaim:
          claimName: myclaim-1



---------------
  336  kubectl apply -f pod.yaml
  
  340  kubectl get pv,pvc
  341  kubectl apply -f pod.yaml
  342  kubectl get pod
  343  kubectl get pv,pvc
  344  kubectl get pod
  345  kubectl delete -f pod.yaml
  346  kubectl get pv,pvc
  347  kubectl delete -f pvc.yaml
  348  kubectl get pv,pvc


Demo3:
-----------

https://github.com/kubernetes/examples/blob/master/staging/volumes/vsphere/simple-statefulset.yaml

apiVersion: v1
kind: Service
metadata:
  name: nginx
  labels:
    app: nginx
spec:
  ports:
  - port: 80
    name: web
  clusterIP: None
  selector:
    app: nginx

---

apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: db
spec:
  serviceName: "nginx"
  replicas: 5
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: k8s.gcr.io/nginx-slim:0.8
        ports:
        - containerPort: 80
          name: web

------------
Demo4: Daemon Set
------------
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: kplabs-daemonset
spec:
  selector:
    matchLabels:
      name: kplabs-all-pods
  template:
    metadata:
      labels:
        name: kplabs-all-pods
    spec:
      containers:
      - name: kplabs-pods
        image: nginx


======
Demo 5
=====

Demo5:
-----------
A ConfigMap stores configuration settings that your Kubernetes Pods consume.

A ConfigMap is a dictionary of key-value pairs that store configuration settings for your applications.

First, create a ConfigMap in your cluster .
Second, consume to ConfigMap in your Pods and use its values.

configmap.yaml:
----------------
kind: ConfigMap 
apiVersion: v1 
metadata:
  name: example-configmap 
data:
  # Configuration values can be set as key-value properties
  key1: mongodb
  key2: mongodb://localhost:27017
  
  
	
kubectl apply -f configmap.yaml
kubectl get configmap
kubectl describe configmap example-configmap


pod.yaml:
---------

apiVersion: v1 
kind: Pod
metadata:
  name: pod-env-var 
spec:
  containers:
    - name: testcon1
      image: nginx:1.7.9 
      env:
        - name: env_var1
          valueFrom:
            configMapKeyRef:
              name: example-configmap
              key: key2
      
$>kubectl apply -f pod.yaml			
$>kubectl exec -it pod-env-var -- /bin/sh

#>env
#exit


-------------
Demo6:
----------======
app.properties:
---------------------
k1=v1
k2=v2
k3=v3

 kubectl create configmap app-config --from-file=app.properties


pod.yaml:
-----------------
apiVersion: v1
kind: Pod
metadata:
  name: configmap-pod
spec:
  containers:
    - name: test-container
      image: nginx
      volumeMounts:
      - name: vol1
        mountPath: /etc/config
  volumes:
    - name: vol1
      configMap:
        name: app-config
  restartPolicy: Never


kubectl exec -it configmap-pod sh

# cd /etc/config
#ls
#exit


-------------
Demo7:
------------
apiVersion: apps/v1
kind: Deployment
metadata:
  creationTimestamp: null
  labels:
    app: nginx-webapp
  name: nginx-webapp
spec:
  replicas: 5
  selector:
    matchLabels:
      app: nginx-webapp
  strategy: {}
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: nginx-webapp
    spec:
      containers:
      - image: busybox
        command: ["/bin/sh"]
        args: ["-c", "while true; do echo echo $(date -u) 'Hi I am from Sidecar container 1' >> /var/log/index.html; sleep 5;done"]
        name: sidecar-container1
        resources: {}
        volumeMounts:
          - name: vol1
            mountPath: /var/log
      - image: busybox
        command: ["/bin/sh"]
        args: ["-c", "while true; do echo echo $(date -u) 'Hi I am from Sidecar container 2' >> /var/log/index.html; sleep 5;done"]
        name: sidecar-container2
        resources: {}
        volumeMounts:
          - name: vol1
            mountPath: /var/log
      - image: nginx
        name: main-container
        resources: {}
        ports:
          - containerPort: 80
        volumeMounts:
          - name: vol1
            mountPath: /usr/share/nginx/html
      dnsPolicy: Default
      volumes:
      - name: vol1
        emptyDir: {}
status: {}

---

apiVersion: v1
kind: Service
metadata:
  name: nginx-webapp
  labels:
    run: nginx-webapp
spec:
  ports:
  - port: 80
    protocol: TCP
  selector:
    app: nginx-webapp
  type: LoadBalancer


Demo8:
----------
apiVersion: batch/v1
kind: Job
metadata:
  name: node-app-job
spec:
  template:
    spec:
      containers:
      - name: node-app-job
        image: alpine   
        command: ["echo", "Welcome to my Node app"]
      restartPolicy: Never
	  
kubectl get pod --watch to see events so that you can observe if there’s an error when creating the job.

kubectl logs <pod-name>

completions & parallelism:
--------------------------
completions: one after other


apiVersion: batch/v1
kind: Job
metadata:
  name: node-app-job
spec:
  completions: 2
  template:
    spec:
      containers:
      - name: node-app-job
        image: alpine
        command: ["echo", "Welcome to my Node app"]
      restartPolicy: Never
	  

apiVersion: batch/v1
kind: Job
metadata:
  name: node-app-job
spec:
  completions: 2
  parallelism: 2
  template:
    spec:
      containers:
      - name: node-app-job
        image: alpine
        command: ["echo", "Welcome to my Node app"]
      restartPolicy: Never
	  
backoffLimit & activeDeadlineSeconds:
-------------------------------------
If your job contains something(ex: command error) that doesn’t allow your job to create successfully, it tries creating pods continuously. When you run kubectl get pods you’ll see several pods with Error status. But using backoffLimit you can limit the number of pods creating continuously.

apiVersion: batch/v1
kind: Job
metadata:
  name: node-app-job
spec:
  backoffLimit: 2
  template:
    spec:
      containers:
      - name: node-app-job
        image: alpine
        command: ["ls", "/data"]
      restartPolicy: Never


activeDeadlineSeconds help us to decide how many seconds should the job runs. To verify this option we set sleep command inside our job to run it for 40 seconds. But using activeDeadlineSeconds: 15 we can terminate the job after 15 seconds.

apiVersion: batch/v1
kind: Job
metadata:
  name: node-app-job
spec:
  activeDeadlineSeconds: 15
  template:
    spec:
      containers:
      - name: node-app-job
        image: alpine
        command: ["sleep", "40"]
      restartPolicy: Never


Demo9:
------------
apiVersion: batch/v1
kind: CronJob
metadata:
  name: node-app-job
spec:
  schedule: "*/1 * * * *"
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: node-app-job
            image: alpine   
            command: ["echo", "Welcome to my Node app"]
          restartPolicy: Never

-------------
Demo11:
--------------
apiVersion: v1
kind: Pod
metadata:
  name: lab1
spec:
  affinity:
    nodeAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 1
        preference:
          matchExpressions:
          - key: dept
            operator: In
            values:
            - trng
            
  containers:
  - name: con1
    image: nginx


Demo10:
---------------
apiVersion: v1
kind: Pod
metadata:
  name: lab1
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: dept
            operator: In
            values:
            - trng
  containers:
  - name: with-node-affinity
    image: nginx

Demo11:
--------------
apiVersion: v1
kind: Pod
metadata:
  name: lab1
spec:
  affinity:
    nodeAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 1
        preference:
          matchExpressions:
          - key: dept
            operator: In
            values:
            - trng
            
  containers:
  - name: con1
    image: nginx


-------------
Demo12:
----------------
apiVersion: v1
kind: Pod
metadata:
  name: pod1
  labels:
    app: frontend
spec:
  containers:
  - name: con1
    image: nginx
--------------------------

apiVersion: v1
kind: Pod
metadata:
  name: pod3
spec:
  affinity:
    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchExpressions:
          - key: app
            operator: In
            values:
            - frontend
        topologyKey: kubernetes.io/hostname
  containers:
  - name: con3
    image: nginx


Metrics Server deploy:
------------------------------
 
 kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml

 kubectl get deployment metrics-server -n kube-system

Demo 13
--------------
nginx-deploy.yaml :
 ----------------------
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    type: dev
  name: nginx-deploy
spec:
  replicas: 1
  selector:
    matchLabels:
      type: dev
  template:
    metadata:
      labels:
        type: dev
    spec:
      containers:
      - image: nginx
        name: nginx
        ports:
        - containerPort: 80
        resources:
          limits:
            memory: 256Mi 
          requests:
            memory: 128Mi 
			



			
hpa-memory.yaml:
---------------------
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: nginx-memory
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: nginx-deploy
  minReplicas: 1
  maxReplicas: 5
  metrics:
  
  - type: Resource
    resource:
      name: memory
      target:
        type: AverageValue
        averageValue: 10Mi


kubectl get hpa


kubectl exec -it podname -- /bin/bash


Stress memory induce:
-------------------------
 cat <(yes | tr \\n x | head -c $((1024*1024*100))) <(sleep 420) | grep n


Demo 14:
=================

nginx-deploy.yaml :
 ----------------------
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    type: dev
  name: nginx-deploy
spec:
  replicas: 1
  selector:
    matchLabels:
      type: dev
  template:
    metadata:
      labels:
        type: dev
    spec:
      containers:
      - image: nginx
        name: nginx
        ports:
        - containerPort: 80
        resources:
          limits:
            cpu: 500m
          requests:
            cpu: 200m
			


hpa1.yaml:
-------------
apiVersion: autoscaling/v1
kind: HorizontalPodAutoscaler
metadata:
  name: nginx-deploy
spec:
  minReplicas: 1
  maxReplicas: 5
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: nginx-deploy 
  targetCPUUtilizationPercentage: 10
  
  kubectl apply -f hpa1.yaml
  
  kubectl get hpa
  
  kubectl exec -it nginx-deploy-67b6f7f794-7cslv -- /bin/bash
  
  Induce stress in pod:
  -------------------------
  
  dd if=/dev/zero of=/dev/null
  
  kubectl get hpa
  
  kubectl describe hpa nginx-deploy
